%%
%% Capítulo 2: Expressões matemáticas
%%

\mychapter{Trabalhos relacionados}
\label{Cap:TrabalhosRelacionados}
Neste capítulo abordaremos os três principais artigos que influenciaram
este trabalho. O primeiro artigo se trata de um algoritmo do estado
da arte do aprendizado por reforço, o segundo e terceiro artigo são
trabalhos que buscaram criar um controlador usando aprendizado por reforço
que rivaliza com os controladores que utilizam a abordagem clássica de controle.
Estes dois últimos artigos foram determinantes para escolher o controlador
estabilizante Vieira. Para este trabalho a parte fundamental do primeiro artigo
foi resumida na seção \ref{sec:muzero}. A forma como os outros dois artigos
utilizaram aprendizado por reforço para criar um controlador de posição e seus
resultados foi escrita na seção \ref{sec:controlador:posicao:aprendizado:reforco}.
Por último a conclusão que me levou a utilizar o controlador estabilizante Vieira
foi escrita na seção \ref{sec:conclusao:revisao:bibliografica}.

\section{MuZero}
\label{sec:muzero}

MuZero é um sistema de aprendizado de máquina que utiliza algoritmos de
aprendizado por reforço e aprendizado supervisionado para
resolver jogos de atari, xadrez e Shogi também conhecido como
xadrez japonês. O MuZero é um sistema com três componentes:
Representação, dinâmica e predição. Representação é uma uma
função $h_{\theta}$  que recebe $t$ observações 
e cria uma Representação interna $s$, $s = h_{\theta}(o_1,...,o_t)$,
onde $o$ é uma observação, onde uma observação pode ser entendia como sendo
um vetor que contem todos os valores dos sensores em um instante $t$.
A predição é uma função $f_{\theta}$ que recebe uma representação $s$
e retorna uma vetor $v$ de tamanho $k$, onde um índice do vetor $v$ representa
uma ação discreta e cada elemento do vetor $v$ é um valor que diz o quão bom
seria tomar a decisão do seu respectivo índice,  $v[k] =  f_{\theta}(s)$.
Uma ação $a$, poderia ser o índice do maior
valor de $v$ ou utilizar a busca em árvore como no método Monte Carlo
através do modelo de dinâmica. O modelo de dinâmica é uma outra função
$g_{\theta}$, que busca prever dado uma ação $a^k$ e um estado $s^{k}$,
o estado $s^{k+1}$ e sua recompensa $r^{k+1}$ associada ao estado,
ou seja, $s^{k+1},r^{k+1}=  g_{\theta}(s^{k},a^k)$.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figuras/muzero.pdf}
    \caption{diagrama dos modelos do MuZero}
\end{figure}

A importância do MuZero para este trabalho não se trata do modelo em si,
mas sim a forma como resolveu o problema, dividindo o sistema em componentes
de forma muito semelhante a como é feito em teoria de controle, onde
o modelo de predição  $f_{\theta}$ pode ser entendido algo
análogo ao controlador, o  modelo dinâmico $g_{\theta}$ como a planta,
e $h_{\theta}$ sendo os sensores, sendo $h_{\theta}$ o modelo
necessário  para que o sistema seja genérico o suficiente para jogar
diferentes jogos. É importante destacar a diferença entre o controlador P.I.D
que toma decisões a partir de $n$ instantes anteriores ($t_n,t_{n-1},...t_{0}$),
já o $f_{\theta}$
treinado a partir do método Monte Carlo toma decisões em $k$ instantes futuros ($t_n,t_{n+1},...t_{n+k}$)
utilizando a função $g_{\theta}$ análoga á planta.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figuras/sistema_classico_controle.pdf}
    \caption{diagrama clássico de controle}
\end{figure}

\section{Controlador de posição e aprendizado por reforço }
\label{sec:controlador:posicao:aprendizado:reforco}
Utilização de aprendizado por reforço para criação de controladores
estabilizantes, foi observado em \cite{farias2020position}
a qual mostrou que o algoritmo \textit{Q-learning} é capaz de aprender
as regras de um controlador com restrições de velocidades linear e angular
para  posicionar o robô em determinada posição, no entanto em seus
experimentos foram necessários 5 milhões de iterações para se ter um modelo
inferior a um controlador que utiliza as leis clássicas de controle.

\subsection{Q-learning}
Uma Função ação-valor determina o quão bom 
para o robô é tomar uma determinada ação $A_t$ quando se está no estado
$S_t$. \textit{Q-learning} é um algoritmo de aprendizado por reforço que
busca aprender uma função ação-valor $Q$ que diretamente aproxima
a função ação-valor ótima $Q^*$ independente do algoritmo utilizado
para selecionar uma ação. O que faz o algoritmo aproximar a função ótima
é o processo de atualização da função $Q$ seguindo a equação do \textit{Q-learning}:

\begin{equation} 
    Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R + \gamma  \max_aQ(S_{t +1},a) - Q(S_t,A_t)]
\end{equation}
onde $S_t$ e $A_t$ é respectivamente estado,ação, $\max_aQ(S_{t +1},a)$ é o valor da
ação de maior valor no estado $S_{t+1}$ e as constantes $\alpha$, $\gamma$ são taxas
de aprendizado que variam de 0 até 1. É muito comum representar a função $Q$
sendo como uma tabela, onde o estado $S$ e ação $A$ são indices. Em contexto de
controladores estabilizantes para robôs moveis, o estado pode ser um
valor de distância até o alvo discretizado e ação pode ser movimentos simples
como girar para esquerda, direita, ir para frente ou ir de rê. Por fim o valor é
número contínuo ou discreto derivado de uma função recompensa que poder
a a variação de distância até o alvo observada após o movimento.
\textit{Q-learning} é apresentado em Algoritmo  \ref{Q-learning:}


\begin{algorithm}[H]
    \label{Q-learning:}
    Parâmetros dos algoritmo: importância de um passo $\alpha \in (0,1]$
    taxa de aprendizado $\gamma \in (0,1] $

    
    \Entrada{ número de epsodios $N_e$; número de passos $N_p$ }
    %% \SetLine
    
    inicialize $Q(s,a)$ arbitrariamente exceto $Q(\text{terminal},\cdot ) = 0$

    \Para {$e \leftarrow 0$ \Ate $N_e$} {
        inicialize o $S$ com o estado inicial $S_0$

        
        \Para {$j \leftarrow 0$ \Ate $N_p$} {
            Escolha a ação $A$ do estado $S$ usando alguma regra derivada de $Q$,
            por exemplo a $\epsilon$-greedy

            Realize a ação $A$ e observe a recompensa $R$ e o novo estado $S'$
            
            $Q(S,A) \leftarrow (S,A) + \alpha[R + \gamma \max_aQ(S',a) - Q(S,A)]$
            
            $S \leftarrow S'$


            \Se {S é terminal}{
                
              $j \leftarrow N_p$  \textbf{ fim do loop de passos}
            }
        }
        
    }
    \Retorna $Q(s,a)$
    \caption{Algoritmo Q-learning}
    
\end{algorithm}

Em \cite{quiroga2022position}
foram utilizado algoritmos variantes do \textit{Q-learning},
como \textit{Deep Q-learning Network} (D.Q.N) e
\textit{Deep Deterministic Policy Gradient} (D.D.P.G),
para encontrar as
regras de um controlador que posicione o robô em determinada
posição e evite de obstáculos usando sensores ultrassônicos.
Apesar do sucesso, o controlador gerado pelos algoritmos não envia
os sinais de velocidades angulares diretamente para o modelo cinemático
do robô, os sinais eram a entrada para o algoritmo de Braitenberg, um
algoritmo que converte sinais de distância de obstáculos em velocidades
lineares e angulares de modo que evite a colisão com obstáculos.
Além do treinamento permitir que o robô
colida com o obstáculo, foi observado que o tempo de treinamento desses modelos
foram de 8,6 e 11,4 horas de treinamento para \textit{Deep Q-learning Network}
e \textit{Deep Deterministic Policy Gradient}, respectivamente.
Segundo o
artigo o melhor o controlador foi o \textit{Deep Deterministic Policy Gradient}.
Ele conseguiu chegar na posição desejada mais rápido do que o
melhor controlador que utiliza as leis clássicas de controle.
Em um ambiente sem obstáculos D.D.P.G chegou 3 segundos mais rápido
do que o melhor controlador que utiliza as leis clássicas de controle.
Em um ambiente com obstáculos a diferença aumentou para
17,3 segundos. No entanto a distância do robô até os obstáculos ficou
inferior a 20 centímetros, ou seja, o modelo de aprendizado encurtou o
caminho ficando mais próximo de bater no obstáculo. É importante dizer que
em ambos os trabalhos foi utilizado um robô de acionamento diferencial.

\subsection{Deep Q-learning e Deep Deterministic Policy Gradient }
\textit{Deep Q-learning Network} é o algoritmo \textit{Q-learning}
aplicado a redes neurais de modo que a função gerada pelo treinamento
dessa rede é uma aproximação da função valor ótima $Q^*$, uma das vantagens
de utilizar redes neurais é o fato de poder utilizar estados contínuos sem a
necessidade de discretização, no entanto o espaço de ação ainda é necessário
ser discreto. Muitas das vezes esse algoritmo é utilizado
tendo como estado $S$ sendo uma imagem que normalmente é preprocessada
antes de enviada para a rede neural. Para ter um treinamento mais estável,
são utilizados duas redes, onde a rede alvo é atualizada usando o parâmetro
de polyak $\rho$ após o fim de uma época $e$.
O algoritmo do \textit{Deep Q-learning Network}
pode ser vista no Algoritmo \ref{Deep:Q-learning:}

\begin{algorithm}[H]
    %% \SetLine
    Parâmetros dos algoritmo:
    importância de um passos $\alpha \in (0,1]$,
    taxa de aprendizado $\gamma \in (0,1]$,
    polyak $\rho \in (0,1]$,
    capacidade da memória $C \in \mathbb{N}$
    tamanho do minibatch $K \in \mathbb{N}$

    \Entrada{ número de epsodios $N_e$; número de passos $N_p$ }

    inicialize os pesos $\theta$ da rede neural que aproxima a função $Q$ com valores aleatórios

    inicialize os pesos $\theta_{\text{target}}  \leftarrow \theta$ rede alvo $Q_{\text{target}}$

    \Para {$e \leftarrow 0$ \Ate $N_e$} {
        inicialize a sequencia $s_0$

        inicialize a sequência preprocessada $\phi_0 \leftarrow \phi(s_0)$ 
        

        \Para {$j \leftarrow 0$ \Ate $N_p$} {
            Escolha a ação $a_t$ do estado processado $phi_t$ usando alguma regra derivada de $Q$,
            por exemplo a $\epsilon$-greedy

            Realize a ação $a_t$ e observe a recompensa $r_t$ e o nova imagem $x_{t+1}$
            
            $s_{t+1}  \leftarrow (s_t, a_t, x_{t+1})$   

            preprocesse a sequencia $\phi_{t+1} \leftarrow \phi(t+1)$
            
            armazene a transição $(\phi_t,a_t,r_t,\phi(t+1))$ na memória $M$

            \Se {$K \ge$ total de transições armazenadas}{
                Selecione $K$ amostras para formar um minibatch de transições

                inicialize o conjunto $y$ de valores desejados
    
                \Para {$k \leftarrow =0$ \Ate $K$}{
    
    
                    \eSe {$s_k$ é terminal}{
                    
                        $y_k \leftarrow r_k$
                      }
                      {
                        $y_k \leftarrow r_k + \gamma \max_aQ(\phi_k,a_k)$
                      }
                    
                }
    
                atualize os pesos $\theta$ baseado no erro $\frac{1}{|K|} \sum_{b =0}^{K}(y -Q(\phi_b,a_b))^2$

        
            }

            \Se {$s_{t+1}$ é terminal}{
                
                $j \leftarrow N_p$  \textbf{ fim do loop de passos}
              }

        }

        ao fim de uma época, atualize os pesos da rede alvo
        $\theta_{\text{target}}  \leftarrow \rho \theta_{\text{target}}  + (1-\rho) \theta$
        
    }
    \Retorna a rede neural $Q_{\text{target}}$
    \caption{Algoritmo Deep Q-learning}
    \label{Deep:Q-learning:}
\end{algorithm}

Como dito anteriormente o espaço das ações do \textit{Deep Q-learning} são discretas,
visando tornar o espaço de ações também contínuas é acrescentando mais
uma rede neural para aproximar a política $\mu$,
desta forma temos um algoritmo baseado na equação \textit{Q-learning} que permite
aprender um sistema de entrada e saída contínua.
\begin{flalign} 
    A_t = \mu(S_t)\\
    Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R + \gamma  Q(S_{t +1},\mu(S_{t+1})) - Q(S_t,A_t)]
\end{flalign}
Enquanto temos uma rede neural $Q$ que busca encontrar a função ação-valor ótima $Q^*$, temos uma
nova rede neural que através do gradiente ascendente buscar encontrar a
política $\mu$ que maximiza a recompensa da função valor aproximada
$Q(S,\mu(S))$.
Perceba que devido a forma como estamos otimizando a rede neural $\mu$,
então o $\max_aQ(S_t,a_k)$ é análogo a $Q(S_t,\mu(S_t))$.
O algoritmo do \textit{Deep Deterministic Policy Gradient}
pode ser vista no Algoritmo \ref{Deep Deterministic Policy Gradient:}. Ambos os
algoritmos \textit{Deep Q-learning} e \textit{Deep Deterministic Policy Gradient} 
foram retirados da documentação da \textit{OpenIA} \cite{SpinningUp2018}, salvo pequenas
adaptações.

\begin{algorithm}[H]
    %% \SetLine
    Parâmetros dos algoritmo:
    importância de um passo $\alpha \in (0,1]$,
    taxa de aprendizado $\gamma \in (0,1]$,
    polyak $\rho \in (0,1]$,
    capacidade da memória $C \in \mathbb{N}$
    tamanho do minibatch $K \in \mathbb{N}$ 

    \Entrada{ número de epsodios $N_e$; número de passos $N_p$ }

    inicialize a memória $M$ com a capacidade $C$

    inicialize os pesos $\theta$ da rede neural que aproxima a função $Q$ com valores aleatórios

    inicialize os pesos $\theta_\mu$ da rede neural que a aproxima da política $\mu(\phi)$ com valores aleatórios

    inicialize os pesos $\theta_{\text{target}}  \leftarrow \theta$ rede alvo $Q_{\text{target}}$

    inicialize os pesos $\theta_{\mu_{\text{target}}}  \leftarrow \theta_\mu$ rede alvo $\mu_{\text{target}}$


    \Para {$e \leftarrow 0$ \Ate $N_e$} {
        inicialize a sequencia $s_0$

        inicialize a sequência preprocessada $\phi_0 \leftarrow \phi(s_0)$ 
        

        \Para {$j \leftarrow 0$ \Ate $N_p$} {
            Escolha a ação $a_t$ do estado processado $phi_t$ usando a rede  $\mu(\phi) + \epsilon$, onde
            $\epsilon$ é um ruído que controla a exploração, $a_t \leftarrow \mu(\phi_t)  + \epsilon$ 

            Realize a ação $a_t$ e observe a recompensa $r_t$ e o nova imagem $x_{t+1}$
            
            $s_{t+1}  \leftarrow (s_t, a_t, x_{t+1})$   

            preprocesse a sequencia $\phi_{t+1} \leftarrow \phi(t+1)$
            
            armazene a transição $(\phi_t,a_t,r_t,\phi(t+1))$ na memória $M$

            \Se {$K \ge$ total de transições armazenadas}{
                Selecione $K$ amostras para formar um minibatch de transições

                inicialize o conjunto $y$ de valores desejados
    
                \Para {$k \leftarrow =0$ \Ate $K$}{
    
    
                    \eSe {$s_k$ é terminal}{
                    
                        $y_k \leftarrow r_k$
                      }
                      {
                        $a_d = \mu_{\text{target}}(\phi_k)$
    
                        $y_k \leftarrow r_k + \gamma Q_{\text{target}}(\phi_k,a_d)$
                      }
                    
                }

                atualize os pesos $\theta$ baseado no erro  $\frac{1}{|K|} \sum_{b =0}^{K} (y -Q(\phi_b,\mu(\phi_b)))^2$
                
                atualize os pesos $\theta_\mu$ baseado no gradiente ascendente:
                $\nabla_{\theta_\mu} \frac{1}{|K|} \sum_{b =0}^{K} Q(\phi_b,\mu(\phi_b))$
            
            }

           
        }

        atualize os pesos da rede alvo $\theta_{\text{target}}  \leftarrow \rho \theta_{\text{target}}  + (1-\rho) \theta$

        atualize os pesos da rede alvo $\theta_{\mu_{\text{target}}}  \leftarrow \rho \theta_{\mu_{\text{target}}}  + (1-\rho)\theta_\mu$

        
    }
    \Retorna as redes neurais $Q_{\text{target}}$, $\mu_{\text{target}}$
    \caption{Algoritmo Deep Deterministic Policy Gradient}
    \label{Deep Deterministic Policy Gradient:}
\end{algorithm}


\section{Conclusão da revisão bibliográfica}
\label{sec:conclusao:revisao:bibliografica}
Os artigos demostram que para um problema simples como posicionar ou estabilizar
um robô de acionamento diferencial em um ambiente sem obstáculos ou com
obstáculos simples, as soluções com aprendizado de máquina por reforço
envolvendo variantes de \textit{Q-learning} visando substituir as
leis clássicas de controle não são interessantes, pois usam muito mais
memória, processamento e apresentam resultados equiparáveis quando não
piores que as soluções utilizando leis clássicas de controle. 