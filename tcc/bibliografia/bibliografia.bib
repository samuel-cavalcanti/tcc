
@article{phaniteja2017deep,
  title        = {A deep reinforcement learning approach for dynamically stable inverse kinematics of humanoid robots},
  author       = {Phaniteja, S and Dewangan, Parijat and Guhan, Pooja and Sarkar, Abhishek and Krishna, K Madhava},
  booktitle    = {2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  pages        = {1818--1823},
  year         = {2017},
  organization = {IEEE},
  address      = {Macau, Macao}
}

@article{cavalcanti2017self,
  title        = {Self-learning in the inverse kinematics of robotic arm},
  author       = {Cavalcanti, Samuel and Santana, Orivaldo},
  booktitle    = {2017 Latin American Robotics Symposium (LARS) and 2017 Brazilian Symposium on Robotics (SBR)},
  pages        = {1--5},
  year         = {2017},
  address      = {Curitiba, Brasil},
  organization = {IEEE}
}

@article{silver2017mastering,
  author     = {David Silver and
                Thomas Hubert and
                Julian Schrittwieser and
                Ioannis Antonoglou and
                Matthew Lai and
                Arthur Guez and
                Marc Lanctot and
                Laurent Sifre and
                Dharshan Kumaran and
                Thore Graepel and
                Timothy P. Lillicrap and
                Karen Simonyan and
                Demis Hassabis},
  title      = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
                Learning Algorithm},
  journal    = {CoRR},
  volume     = {abs/1712.01815},
  year       = {2017},
  url        = {http://arxiv.org/abs/1712.01815},
  eprinttype = {arXiv},
  eprint     = {1712.01815},
  timestamp  = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  pages      = {1--19}
}

@article{schrittwieser2020mastering,
  author     = {Julian Schrittwieser and
                Ioannis Antonoglou and
                Thomas Hubert and
                Karen Simonyan and
                Laurent Sifre and
                Simon Schmitt and
                Arthur Guez and
                Edward Lockhart and
                Demis Hassabis and
                Thore Graepel and
                Timothy P. Lillicrap and
                David Silver},
  title      = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal    = {CoRR},
  volume     = {abs/1911.08265},
  year       = {2019},
  url        = {http://arxiv.org/abs/1911.08265},
  eprinttype = {arXiv},
  eprint     = {1911.08265},
  timestamp  = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1911-08265.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{farias2020position,
  title    = {Position control of a mobile robot using reinforcement learning},
  journal  = {IFAC-PapersOnLine},
  volume   = {53},
  number   = {2},
  pages    = {17393-17398},
  year     = {2020},
  note     = {21st IFAC World Congress},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2020.12.2093},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896320327440},
  author   = {G. Farias and G. Garcia and G. Montenegro and E. Fabregas and S. Dormido-Canto and S. Dormido},
  keywords = {Control Education, Mobile Robot, Position Control, Reinforcement Learning},
  abstract = {Robotics has been introduced in education at all levels during the last years. In particular, the application of mobile robots for teaching automatic control is becoming more popular in engineering because of the attractive experiments that can be performed. This paper presents the design, development, and implementation of an algorithm to control the position of a wheeled mobile robot using Reinforcement Learning in an advanced 3D simulation environment. In this approach, the learning process occurs when the agent makes some actions in the environment to get some rewards. Trying to make a balance between the new information of the environment and the current knowledge about it. In this way, the algorithm is divided into two phases: 1) the learning stage, and 2) the operational stage. In the first stage, the robot learns how to reach a known destination point from its current position. To do it, it uses the information of the environment and the rewards, to build a learning matrix that is used later during the operational stage. The main advantage of this algorithm concerning traditional control algorithms is that the learning process is carried out automatically with a recursive procedure and the result is a controller that can make the specific task, without the need for a dynamic model. Its main drawback is that the learning stage can take a long time to finish and it depends on the hardware resources of the computer used during the learning process.}
}
@article{quiroga2022position,
  author         = {Quiroga, Francisco and Hermosilla, Gabriel and Farias, Gonzalo and Fabregas, Ernesto and Montenegro, Guelis},
  title          = {Position Control of a Mobile Robot through Deep Reinforcement Learning},
  journal        = {Applied Sciences},
  volume         = {12},
  year           = {2022},
  number         = {14},
  article-number = {7194},
  url            = {https://www.mdpi.com/2076-3417/12/14/7194},
  issn           = {2076-3417},
  abstract       = {This article proposes the use of reinforcement learning (RL) algorithms to control the position of a simulated Kephera IV mobile robot in a virtual environment. The simulated environment uses the OpenAI Gym library in conjunction with CoppeliaSim, a 3D simulation platform, to perform the experiments and control the position of the robot. The RL agents used correspond to the deep deterministic policy gradient (DDPG) and deep Q network (DQN), and their results are compared with two control algorithms called Villela and IPC. The results obtained from the experiments in environments with and without obstacles show that DDPG and DQN manage to learn and infer the best actions in the environment, allowing us to effectively perform the position control of different target points and obtain the best results based on different metrics and indices.},
  doi            = {10.3390/app12147194}
}

@article{SpinningUp2018,
  author = {Achiam, Joshua},
  title  = {{Spinning Up in Deep Reinforcement Learning}},
  year   = {2018},
  url    = {https://spinningup.openai.com/}
}

@article{rooban2021coppeliasim,
  title     = {CoppeliaSim (formerly V-REP): a Versatile and Scalable Robot Simulation Framework},
  author    = {E. Rohmer and S. P. N. Singh and M. Freese},
  journal   = {Materials Today: Proceedings},
  year      = {2013},
  booktitle = {Proc. of The International Conference on Intelligent Robots and Systems (IROS)},
  url       = {www.coppeliarobotics.com}
}

@mastersthesis{oliveira2022projeto,
  title   = {Projeto e implementa{\c{c}}{\~a}o do sistema mec{\^a}nico e do acionamento eletr{\^o}nico de pot{\^e}ncia de um Andador Rob{\'o}tico Inteligente},
  author  = {Oliveira, Alberto Tavares de},
  type    = {{Bacharelado em Engenharia Mecatrônica} Trabalho de conclusão de curso},
  year    = {2022},
  school  = {Universidade Federal do Rio Grande do Norte},
  address = {Natal, Rio Grande do Norte}
}
@mastersthesis{vieira2006controle,
  title     = {Controle din{\^a}mico de rob{\^o}s m{\'o}veis com acionamento diferencial},
  author    = {Vieira, Frederico Carvalho},
  year      = {2006},
  school    = {Universidade Federal do Rio Grande do Norte},
  address   = {Natal, Rio Grande do Norte},
  pagetotal = {106}
}

@book{siegwart2011introduction,
  title     = {Introduction to autonomous mobile robots},
  author    = {Siegwart, Roland and Nourbakhsh, Illah Reza and Scaramuzza, Davide},
  year      = {2011},
  publisher = {MIT press}
}

@book{mitchell1997machine,
  title     = {Machine learning},
  author    = {Mitchell, Tom M and Mitchell, Tom M},
  volume    = {1},
  number    = {9},
  year      = {1997},
  publisher = {McGraw-hill New York}
}

@book{sutton2018reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press},
  address = {Cambridge, Massachusetts}
}

@book{chollet2021deep,
  title     = {Deep learning with Python},
  author    = {Chollet, Francois},
  year      = {2021},
  publisher = {Simon and Schuster},
  address   = {Shelter Island, NY 11964}
}

@book{trask2019grokking,
  title     = {Grokking deep learning},
  author    = {Trask, Andrew W},
  year      = {2019},
  publisher = {Simon and Schuster}
}


@book{ogataengenharia,
  title     = {Engenharia de Controle Moderno},
  author    = {Katsuhiko Ogata},
  volume    = {5},
  year      = {2010},
  publisher = {S{\~a}o Paulo: Pretince Hall}
}



@book{ogata2010modern,
  title     = {Modern control engineering},
  author    = {Ogata, Katsuhiko and others},
  volume    = {5},
  year      = {2010},
  publisher = {Prentice hall Upper Saddle River, NJ}
}